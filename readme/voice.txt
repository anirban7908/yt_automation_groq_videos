File: voice.py

1. What it does?
This file is the "Narrator" and "Timekeeper" of your automation pipeline. Its primary job is to take the text script generated by the Brain and convert it into a high-quality, human-like voiceover using Microsoft Edge's Neural TTS engine.

Crucially, it also handles **Visual Pacing**. It measures the exact length of the generated audio (e.g., 5.2 seconds) and mathematically calculates how many images are needed to cover that duration so the video flows smoothly without static images sitting on screen for too long.

2. What are the libraries used?
Here are the libraries imported in this file and why they are used:

* edge_tts
  - Definition: A Python library that interfaces with Microsoft Edge's online Text-to-Speech service.
  - Why used here?: It provides the "en-US-GuyNeural" voice, which is free and sounds significantly more realistic than standard robotic voices. It also allows for speed adjustments (e.g., `rate="+10%"`).

* mutagen (specifically MP3)
  - Definition: A library for handling audio metadata.
  - Why used here?: This is critical for timing. After generating the audio file, the code needs to know *exactly* how long it is (down to the millisecond) to plan the visuals. `MP3(path).info.length` provides this precise duration.

* math
  - Definition: Standard Python library for mathematical functions.
  - Why used here?: Used for the `ceil` (ceiling) function. If a scene is 5 seconds long and we want max 4 seconds per image, `math.ceil(5 / 4)` rounds 1.25 up to 2 images. This ensures we never have partial images.

* os
  - Definition: Standard Python library for file system operations.
  - Why used here?: Used to construct file paths (`os.path.join`) so the audio files are saved correctly inside the specific video's folder.

* core.db_manager.DBManager
  - Definition: Your custom class that handles database connections.
  - Why used here?: It fetches tasks with the status "scripted" and, after generating audio and calculating timing, updates the task with the new `image_count` and sets the status to "voiced".

3. Which is the main function and what does it do?

Main Function: generate_audio(self)

Description:
This function manages the transformation from text to sound.
1. Fetching: It asks the database for a task where `status: "scripted"`.
2. Looping: It iterates through every scene in the script.
3. Generating: It calls `edge_tts` to speak the text for that scene and saves it as an MP3 file (e.g., `voice_0.mp3`).
4. Measuring: It uses `mutagen` to check the file's length (e.g., 7.5 seconds).
5. Calculating (The Fix): It applies the logic `math.ceil(duration / 4.0)` to determine exactly how many images are needed for this specific audio clip.
6. Saving: It updates the script data in the database with the audio path, duration, and the calculated image count, then marks the status as "voiced".

Helper Functions & Components Discussion:

* communicate = edge_tts.Communicate(text, "en-US-GuyNeural", rate="+10%")
  - Purpose: The voice generation command.
  - Why?: It selects the specific voice persona and applies a 10% speed boost to make the narration sound more energetic and engaging for YouTube Shorts.

* The Time-Based Calculation Logic
  - Code: `required_images = math.ceil(duration / 4.0)`
  - Purpose: Dynamic Pacing.
  - How it works:
    - If audio is 3.0s -> 3/4 = 0.75 -> Rounds up to **1 image**.
    - If audio is 7.0s -> 7/4 = 1.75 -> Rounds up to **2 images** (each plays for 3.5s).
    - If audio is 9.0s -> 9/4 = 2.25 -> Rounds up to **3 images** (each plays for 3.0s).
  - Why?: This prevents a single image from being stuck on the screen for 10 seconds while the narrator keeps talking, which would be boring for the viewer.